{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HCj3HFl9S9QrXLD6oR05kXYmwK_zVQ-Y",
      "authorship_tag": "ABX9TyPxzkp5x+n+vYBAF3M8vnOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzuupp/Object_Detection-Segmentation/blob/main/pytorch-objdet/Pytorch_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection 구현"
      ],
      "metadata": {
        "id": "r-oPzwIvY3TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZUbtQhw-Yd74"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import functional as FT\n",
        "from torchvision import transforms as T\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from pycocotools.coco import COCO\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Vg8dVyb2Y0Ix"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "E7LjwOcWay2t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Aqua_data_train_url  = '/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.coco/train'\n",
        "Aqua_data_valid_url = '/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.coco/valid'\n",
        "Aqua_data_test_url = '/content/drive/MyDrive/Aquarium Combined.v2-raw-1024.coco/test'"
      ],
      "metadata": {
        "id": "Bl0T641Jc03m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train = False):\n",
        "    if train :\n",
        "        transform = A.Compose([\n",
        "            A.Resize(600, 600), # our input size can be 600px\n",
        "            A.HorizontalFlip(p = 0.3), # 30% 확률로 좌우 뒤집기.\n",
        "            A.VerticalFlip(p = 0.3),   # 30% 확률로 위아래 뒤집기.\n",
        "            A.RandomBrightnessContrast(p = 0.1), # 10% 확률로 밝기/대비 무작위 변화.\n",
        "            A.ColorJitter(p = 0.1), # 10% 확률로 색상/채도/명도 변화\n",
        "            ToTensorV2() # NumPy => PyTorch Tensor 변환\n",
        "        ], bbox_params = A.BboxParams(format = 'coco'))  #객체 탐지는 바운딩박스도 함께 변환해야함."
      ],
      "metadata": {
        "id": "CARjToGFiiPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}